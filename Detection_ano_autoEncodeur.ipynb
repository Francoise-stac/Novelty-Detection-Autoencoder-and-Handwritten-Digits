{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1er assaie"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "\n",
        "\n",
        "# Charger le jeu de données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normaliser les pixels entre 0 et 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "# Diviser le jeu de données en ensembles d'entraînement et de test\n",
        "X_train, X_test, _, _ = train_test_split(x_train, x_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplatir les images et les normaliser\n",
        "X_train = X_train.reshape((len(X_train), 28, 28, 1))\n",
        "X_test = X_test.reshape((len(X_test), 28, 28, 1))\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Créer un autoencodeur convolutionnel\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Entraîner l'autoencodeur sur le jeu d'entraînement (sans anomalies)\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_test, X_test))\n",
        "\n",
        "# Encoder de l'autoencodeur (utilisé comme détecteur d'anomalies)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Obtenir les représentations encodées des images d'entraînement\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "\n",
        "# Aplatir les représentations encodées\n",
        "X_train_encoded_flat = X_train_encoded.reshape((len(X_train_encoded), np.prod(X_train_encoded.shape[1:])))\n",
        "\n",
        "# Entraîner le modèle One-Class SVM sur les représentations encodées\n",
        "clf = OneClassSVM(nu=0.1)\n",
        "clf.fit(X_train_encoded_flat)\n",
        "\n",
        "# Prédiction sur le jeu de test\n",
        "X_test_encoded = encoder.predict(X_test)\n",
        "X_test_encoded_flat = X_test_encoded.reshape((len(X_test_encoded), np.prod(X_test_encoded.shape[1:])))\n",
        "y_pred = clf.predict(X_test_encoded_flat)\n",
        "\n",
        "# Étiquettes réelles du jeu de test\n",
        "y_true = np.ones_like(y_pred)\n",
        "\n",
        "# Évaluer la précision sur le jeu de test\n",
        "accuracy_test = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy on test set: {accuracy_test}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-11-24 11:31:58.281298: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-11-24 11:31:58.284403: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-24 11:31:58.337764: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-11-24 11:31:58.337807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-11-24 11:31:58.338919: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-24 11:31:58.345656: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-24 11:31:58.346243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-24 11:31:59.488735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n2023-11-24 11:32:06.835122: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11490434/11490434 [==============================] - 1s 0us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 1/10\n375/375 [==============================] - 11s 27ms/step - loss: 0.0493 - val_loss: 0.0042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/10\n375/375 [==============================] - 10s 25ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/10\n375/375 [==============================] - 10s 25ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/10\n375/375 [==============================] - 10s 26ms/step - loss: 0.0040 - val_loss: 0.0040\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1500/1500 [==============================] - 3s 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n375/375 [==============================] - 1s 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nAccuracy on test set: 0.9039166666666667\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1700825711178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting tensorflow\n  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow)\n  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting astunparse>=1.6.0 (from tensorflow)\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting flatbuffers>=23.5.26 (from tensorflow)\n  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\nCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\nCollecting google-pasta>=0.1.1 (from tensorflow)\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow)\n  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow)\n  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting ml-dtypes~=0.2.0 (from tensorflow)\n  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (1.25.0)\nCollecting opt-einsum>=2.3.2 (from tensorflow)\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (67.8.0)\nRequirement already satisfied: six>=1.12.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nCollecting termcolor>=1.1.0 (from tensorflow)\n  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nRequirement already satisfied: typing-extensions>=3.6.6 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nCollecting wrapt<1.15,>=1.11.0 (from tensorflow)\n  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorflow) (1.43.0)\nCollecting tensorboard<2.16,>=2.15 (from tensorflow)\n  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\nCollecting grpcio<2.0,>=1.24.3 (from tensorflow)\n  Downloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.20.0)\nCollecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\nRequirement already satisfied: markdown>=2.6.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow)\n  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.3.6)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (1.26.16)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nInstalling collected packages: libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, opt-einsum, ml-dtypes, keras, h5py, grpcio, google-pasta, gast, astunparse, absl-py, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.43.0\n    Uninstalling grpcio-1.43.0:\n      Successfully uninstalled grpcio-1.43.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nray 2.0.0 requires grpcio<=1.43.0,>=1.42.0; python_version >= \"3.10\", but you have grpcio 1.59.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-2.0.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 opt-einsum-3.3.0 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 wrapt-1.14.1\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700825492051
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2eme essaie"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normaliser le jeu de données MNIST"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "\n",
        "# Charger le jeu de données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "x_train.shape\n",
        "\n",
        "# Normaliser les pixels entre 0 et 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "# Diviser le jeu de données en ensembles d'entraînement et de test\n",
        "x_train, X_test, _, _ = train_test_split(x_train, x_train, test_size=10000, random_state=42)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(x_train, x_train, test_size=12000, random_state=42)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "((38000, 28, 28), (10000, 28, 28), (12000, 28, 28))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700830354866
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### générer les images aléatoire"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ici, j'utilise des images aléatoires, mais vous pouvez les remplacer par votre propre dataset\n",
        "random_images = np.random.rand(10000, 28, 28)\n",
        "# anomalies = random_images.reshape((len(random_images), np.prod(random_images.shape[1:])))\n",
        "\n",
        "anomalies = random_images.astype('float32') / 255.0\n",
        "anomalies.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "(10000, 28, 28)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700830377151
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Concaténer le jeu de données MNIST avec le jeu d'anomalies\n",
        "x_combined = np.concatenate([X_test, anomalies])\n",
        "x_combined.shape\n",
        "\n",
        "# Aplatir les images et les normaliser\n",
        "X_train = X_train.reshape((len(X_train), 28, 28, 1))\n",
        "X_val = X_val.reshape((len(X_val), 28, 28, 1))\n",
        "X_test = x_combined.reshape((len(x_combined), 28, 28, 1))\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "((38000, 28, 28, 1), (20000, 28, 28, 1), (12000, 28, 28, 1))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700830444128
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Créer un autoencodeur convolutionnel\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700830456915
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Encoder de l'autoencodeur (utilisé comme détecteur d'anomalies)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Obtenir les représentations encodées des images d'entraînement\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "\n",
        "# Aplatir les représentations encodées\n",
        "X_train_encoded_flat = X_train_encoded.reshape((len(X_train_encoded), np.prod(X_train_encoded.shape[1:])))\n",
        "\n",
        "# Entraîner le modèle One-Class SVM sur les représentations encodées\n",
        "clf = OneClassSVM(nu=0.1)\n",
        "clf.fit(X_train_encoded_flat)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "\n",
        "# Charger le jeu de données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Ici, j'utilise des images aléatoires, mais vous pouvez les remplacer par votre propre dataset\n",
        "random_images = np.random.rand(10000, 28, 28)\n",
        "anomalies = random_images.astype('float32') / 255.0\n",
        "anomalies.shape   # Aplatir les images et les normaliser\n",
        "X_train = x_train.reshape((len(x_train), 28, 28, 1))\n",
        "anomalies = anomalies.reshape((len(anomalies), 28, 28, 1))\n",
        "X_train = X_train / 255.0\n",
        "anomalies = anomalies / 255.0\n",
        "\n",
        "# Diviser le jeu de données en ensembles d'entraînement (38 000 images), de validation (12 000 images) et de test (1000 MNIST + 10 000 générées)\n",
        "X_train, X_val = train_test_split(X_train, test_size=12000, random_state=42)\n",
        "X_train, X_test = train_test_split(X_train, test_size=10000, random_state=42)\n",
        "# X_test_anomalies, _ = train_test_split(anomalies, test_size=10000, random_state=42)\n",
        "X_test_combined = np.concatenate([X_test, anomalies])\n",
        "X_test_combined.shape, X_train.shape, X_val.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "((20000, 28, 28, 1), (38000, 28, 28, 1), (12000, 28, 28, 1))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700831492069
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Créer un autoencodeur convolutionnel\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=128, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Encoder de l'autoencodeur (utilisé comme détecteur d'anomalies)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Obtenir les représentations encodées des images d'entraînement\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "\n",
        "# Aplatir les représentations encodées\n",
        "X_train_encoded_flat = X_train_encoded.reshape((len(X_train_encoded), np.prod(X_train_encoded.shape[1:])))\n",
        "\n",
        "# Entraîner le modèle One-Class SVM sur les représentations encodées\n",
        "clf = OneClassSVM(nu=0.1)\n",
        "clf.fit(X_train_encoded_flat)\n",
        "\n",
        "# Prédiction sur l'ensemble de test (MNIST + anomalies)\n",
        "X_test_mnist_encoded = encoder.predict(X_test_mnist)\n",
        "X_test_mnist_encoded_flat = X_test_mnist_encoded.reshape((len(X_test_mnist_encoded), np.prod(X_test_mnist_encoded.shape[1:])))\n",
        "y_pred_mnist = clf.predict(X_test_mnist_encoded_flat)\n",
        "\n",
        "X_test_anomalies_encoded = encoder.predict(X_test_anomalies)\n",
        "X_test_anomalies_encoded_flat = X_test_anomalies_encoded.reshape((len(X_test_anomalies_encoded), np.prod(X_test_anomalies_encoded.shape[1:])))\n",
        "y_pred_anomalies = clf.predict(X_test_anomalies_encoded_flat)\n",
        "\n",
        "# Étiquettes réelles de l'ensemble de test MNIST\n",
        "y_true_mnist = np.ones_like(y_pred_mnist)\n",
        "\n",
        "# Étiquettes réelles de l'ensemble de test anomalies\n",
        "y_true_anomalies = -1 * np.ones_like(y_pred_anomalies)\n",
        "\n",
        "# Concaténer les prédictions et les étiquettes réelles\n",
        "y_pred = np.concatenate([y_pred_mnist, y_pred_anomalies])\n",
        "y_true = np.concatenate([y_true_mnist, y_true_anomalies])\n",
        "\n",
        "# Évaluer la précision sur l'ensemble de test\n",
        "accuracy_test = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy on test set: {accuracy_test}\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700830897381
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "\n",
        "\n",
        "# Charger le jeu de données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Ici, j'utilise des images aléatoires, mais vous pouvez les remplacer par votre propre dataset\n",
        "random_images = np.random.rand(10000, 28, 28)\n",
        "anomalies = random_images.astype('float32') / 255.0\n",
        "anomalies.shape   # Aplatir les images et les normaliser\n",
        "X_train = x_train.reshape((len(x_train), 28, 28, 1))\n",
        "anomalies = anomalies.reshape((len(anomalies), 28, 28, 1))\n",
        "X_train = X_train / 255.0\n",
        "anomalies = anomalies / 255.0\n",
        "\n",
        "# Diviser le jeu de données en ensembles d'entraînement (38 000 images), de validation (12 000 images) et de test (1000 MNIST + 10 000 générées)\n",
        "X_train, X_temp = train_test_split(X_train, test_size=12000, random_state=42)\n",
        "X_train, X_test = train_test_split(X_temp, test_size=10000, random_state=42)\n",
        "\n",
        "# Concaténer les images de test MNIST avec les anomalies générées\n",
        "X_test_combined = np.concatenate([X_test, anomalies])\n",
        "\n",
        "X_train.shape, X_test_combined.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "((2000, 28, 28, 1), (20000, 28, 28, 1))"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700832060098
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp = train_test_split(X_train, test_size=12000, random_state=42)\n",
        "X_val, X_test = train_test_split(X_temp, test_size=10000, random_state=42)\n",
        "X_train.shape,X_val.shape, X_test.shape"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "test_size=12000 should be either positive and smaller than the number of samples 2000 or a float in the (0, 1) range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m X_val, X_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_train\u001b[38;5;241m.\u001b[39mshape,X_val\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2181\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2173\u001b[0m train_size_type \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(train_size)\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2176\u001b[0m     test_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m test_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2178\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m test_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2179\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (test_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2180\u001b[0m ):\n\u001b[0;32m-> 2181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_size, n_samples)\n\u001b[1;32m   2185\u001b[0m     )\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2188\u001b[0m     train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2189\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2191\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2192\u001b[0m ):\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size, n_samples)\n\u001b[1;32m   2197\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: test_size=12000 should be either positive and smaller than the number of samples 2000 or a float in the (0, 1) range"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700832372907
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un autoencodeur convolutionnel\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=128, shuffle=True, validation_split=0.1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/10\n15/15 [==============================] - 1s 36ms/step - loss: 0.5951 - val_loss: 0.4776\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.4528 - val_loss: 0.4053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.3664 - val_loss: 0.3188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.2834 - val_loss: 0.2553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/10\n15/15 [==============================] - 0s 27ms/step - loss: 0.2400 - val_loss: 0.2333\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.2253 - val_loss: 0.2249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.2167 - val_loss: 0.2159\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/10\n15/15 [==============================] - 0s 27ms/step - loss: 0.2095 - val_loss: 0.2090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/10\n15/15 [==============================] - 0s 27ms/step - loss: 0.2037 - val_loss: 0.2029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/10\n15/15 [==============================] - 0s 25ms/step - loss: 0.1983 - val_loss: 0.1991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "<keras.src.callbacks.History at 0x7fb41e73e530>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700832148112
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "(2000, 28, 28, 1)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700832181562
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Encoder de l'autoencodeur (utilisé comme détecteur d'anomalies)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Obtenir les représentations encodées des images d'entraînement\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "\n",
        "# Aplatir les représentations encodées\n",
        "X_train_encoded_flat = X_train_encoded.reshape((len(X_train_encoded), np.prod(X_train_encoded.shape[1:])))\n",
        "\n",
        "# Entraîner le modèle One-Class SVM sur les représentations encodées\n",
        "clf = OneClassSVM(nu=0.1)\n",
        "clf.fit(X_train_encoded_flat)\n",
        "\n",
        "# Prédiction sur l'ensemble de test (MNIST + anomalies)\n",
        "X_test_combined_encoded = encoder.predict(X_test_combined)\n",
        "X_test_combined_encoded_flat = X_test_combined_encoded.reshape((len(X_test_combined_encoded), np.prod(X_test_combined_encoded.shape[1:])))\n",
        "y_pred_combined = clf.predict(X_test_combined_encoded_flat)\n",
        "\n",
        "# Étiquettes réelles de l'ensemble de test (MNIST + anomalies)\n",
        "y_true_combined = np.concatenate([np.ones(len(X_test)), -1 * np.ones(len(anomalies))])\n",
        "\n",
        "# Évaluer la précision sur l'ensemble de test\n",
        "accuracy_test = accuracy_score(y_true_combined, y_pred_combined)\n",
        "print(f\"Accuracy on combined test set: {accuracy_test}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/10\n15/15 [==============================] - 1s 39ms/step - loss: 0.6759 - val_loss: 0.6128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/10\n15/15 [==============================] - 0s 25ms/step - loss: 0.5114 - val_loss: 0.4491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.4096 - val_loss: 0.3625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/10\n15/15 [==============================] - 0s 25ms/step - loss: 0.3182 - val_loss: 0.2777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.2526 - val_loss: 0.2372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/10\n15/15 [==============================] - 0s 25ms/step - loss: 0.2240 - val_loss: 0.2203\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.2119 - val_loss: 0.2105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.2038 - val_loss: 0.2036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/10\n15/15 [==============================] - 0s 25ms/step - loss: 0.1971 - val_loss: 0.1980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/10\n15/15 [==============================] - 0s 26ms/step - loss: 0.1909 - val_loss: 0.1909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n63/63 [==============================] - 0s 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n625/625 [==============================] - 2s 3ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nAccuracy on combined test set: 0.9453\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700831885408
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "\n",
        "# Charger le jeu de données MNIST\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Ici, j'utilise des images aléatoires, mais vous pouvez les remplacer par votre propre dataset\n",
        "random_images = np.random.rand(10000, 28, 28)\n",
        "anomalies = random_images.astype('float32') / 255.0\n",
        "anomalies.shape   # Aplatir les images et les normaliser\n",
        "X_train = x_train.reshape((len(x_train), 28, 28, 1))\n",
        "anomalies = anomalies.reshape((len(anomalies), 28, 28, 1))\n",
        "X_train = X_train / 255.0\n",
        "anomalies = anomalies / 255.0\n",
        "\n",
        "# Diviser le jeu de données en ensembles d'entraînement (38 000 images MNIST), de validation (12 000 images MNIST) et de test (10 000 MNIST + 10 000 générées)\n",
        "X_train, X_val = train_test_split(X_train, test_size=12000, random_state=42)\n",
        "X_train, X_test = train_test_split(X_train, test_size=10000, random_state=42)\n",
        "\n",
        "# Concaténer les images de test MNIST avec les anomalies générées\n",
        "X_test_combined = np.concatenate([X_test, anomalies])\n",
        "\n",
        "# Créer un autoencodeur convolutionnel\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=128, shuffle=True, validation_data=(X_val, X_val))\n",
        "\n",
        "# Encoder de l'autoencodeur (utilisé comme détecteur d'anomalies)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Obtenir les représentations encodées des images d'entraînement\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "\n",
        "# Aplatir les représentations encodées\n",
        "X_train_encoded_flat = X_train_encoded.reshape((len(X_train_encoded), np.prod(X_train_encoded.shape[1:])))\n",
        "\n",
        "# Entraîner le modèle One-Class SVM sur les représentations encodées\n",
        "clf = OneClassSVM(nu=0.1)\n",
        "clf.fit(X_train_encoded_flat)\n",
        "\n",
        "# COMPARE la mse du X_train_encoded_flat et le xtrain_flat, \n",
        "# si endessous d'un certain seuil c'est une donnee normale si en dessus c'est une anomalie  \n",
        "\n",
        "\n",
        "# Prédiction sur l'ensemble de test (MNIST + anomalies)\n",
        "X_test_combined_encoded = encoder.predict(X_test_combined)\n",
        "X_test_combined_encoded_flat = X_test_combined_encoded.reshape((len(X_test_combined_encoded), np.prod(X_test_combined_encoded.shape[1:])))\n",
        "y_pred_combined = clf.predict(X_test_combined_encoded_flat)\n",
        "\n",
        "# Étiquettes réelles de l'ensemble de test (MNIST + anomalies)\n",
        "y_true_combined = np.concatenate([np.ones(10000), -1 * np.ones(10000)])\n",
        "\n",
        "# Évaluer la précision sur l'ensemble de test\n",
        "accuracy_test = accuracy_score(y_true_combined, y_pred_combined)\n",
        "print(f\"Accuracy on combined test set: {accuracy_test}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/50\n297/297 [==============================] - 9s 28ms/step - loss: 0.2525 - val_loss: 0.1653\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1488 - val_loss: 0.1373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1316 - val_loss: 0.1273\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1240 - val_loss: 0.1217\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1193 - val_loss: 0.1174\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1160 - val_loss: 0.1155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1135 - val_loss: 0.1123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1115 - val_loss: 0.1107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1099 - val_loss: 0.1094\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1087 - val_loss: 0.1084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 11/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1076 - val_loss: 0.1078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 12/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1066 - val_loss: 0.1064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 13/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1058 - val_loss: 0.1056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 14/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1050 - val_loss: 0.1048\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 15/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1043 - val_loss: 0.1044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 16/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1038 - val_loss: 0.1036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 17/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1032 - val_loss: 0.1035\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 18/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1027 - val_loss: 0.1025\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 19/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1022 - val_loss: 0.1022\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 20/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1018 - val_loss: 0.1018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 21/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1014 - val_loss: 0.1012\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 22/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1010 - val_loss: 0.1008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 23/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1007 - val_loss: 0.1004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 24/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1003 - val_loss: 0.1004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 25/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.1001 - val_loss: 0.0998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 26/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0997 - val_loss: 0.0998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 27/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0995 - val_loss: 0.0996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 28/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0992 - val_loss: 0.0995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 29/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0990 - val_loss: 0.0993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 30/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0988 - val_loss: 0.0989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 31/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0986 - val_loss: 0.0986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 32/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0983 - val_loss: 0.0982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 33/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0981 - val_loss: 0.0983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 34/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0979 - val_loss: 0.0979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 35/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0978 - val_loss: 0.0980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 36/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0975 - val_loss: 0.0974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 37/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0974 - val_loss: 0.0972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 38/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0972 - val_loss: 0.0973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 39/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0970 - val_loss: 0.0973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 40/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0969 - val_loss: 0.0968\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 41/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0968 - val_loss: 0.0974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 42/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0967 - val_loss: 0.0969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 43/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0965 - val_loss: 0.0966\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 44/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0964 - val_loss: 0.0967\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 45/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0962 - val_loss: 0.0962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 46/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0962 - val_loss: 0.0962\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 47/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0960 - val_loss: 0.0971\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 48/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0959 - val_loss: 0.0960\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 49/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0958 - val_loss: 0.0958\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 50/50\n297/297 [==============================] - 8s 27ms/step - loss: 0.0958 - val_loss: 0.0959\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1188/1188 [==============================] - 2s 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n625/625 [==============================] - 1s 2ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nAccuracy on combined test set: 0.9509\n"
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700833270478
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "fr"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}